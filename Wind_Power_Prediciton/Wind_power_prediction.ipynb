{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.pexels.com/photos/532192/pexels-photo-532192.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)\n",
    "\n",
    "# Wind power forecasting\n",
    "Wind energy is the energy of the wind transformed into useful energy through wind turbines. This renewable energy source is widely used because it is an alternative to fossil energy, it is clean, does not produce greenhouse gases and can generally be used in various locations, however there are still some environmental and social problems attached, such as soil compaction and the noise emitted by the blades. In addition, wind energy suffers a lot from the fluctuation of winds and, therefore, doors are opened for the application of Machine Learning models to be used to make generation forecasts. Finally, this project aims to forecast the wind power generation of a wind turbine located in Germany with historical data from 2011 to the end of 2021.\n",
    "\n",
    "# Dictionary (Column)\n",
    "- dt: Time series with timestep of 15 minutes.\n",
    "- MW: Wind power (MW).\n",
    "   \n",
    "# References\n",
    "- [kaggle dataset](https://www.kaggle.com/datasets/l3llff/wind-power)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Importing Libraries and Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import Conv1D,  MaxPooling1D\n",
    "\n",
    "import os\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the dataset:\n",
    "dado_horarios = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the first five rows:\n",
    "dado_horarios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some information about data type and memory:\n",
    "dado_horarios.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the type of the Time series column:\n",
    "dado_horarios['dt'] = pd.to_datetime(dado_horarios['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dt column as index:\n",
    "dado_horarios.set_index('dt', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some decriptive statistics:\n",
    "dado_horarios.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data:\n",
    "dado_horarios.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there is no missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating information:\n",
    "dado_horarios['month'] = dado_horarios.index.month\n",
    "dado_horarios['year'] = dado_horarios.index.year\n",
    "\n",
    "group = dado_horarios.groupby([\"year\", \"month\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the monthly average Wind Power by year:\n",
    "fig = px.line(data_frame=group, x='month', y='MW', color='year', title=\"Monthly Average Wind power generation by year\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Compared to the average energy produced in 2011, the energy produced in 2021 is much bigger for every month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma da s√©rie temporal:\n",
    "fig, ax = plt.subplots(ncols=4, nrows=3, sharex=False, sharey=False, figsize=(25, 20))\n",
    "\n",
    "sns.histplot(x=group[group['year'] == 2011]['MW'], kde=True, ax=ax[0, 0])\n",
    "sns.histplot(x=group[group['year'] == 2012]['MW'], kde=True, ax=ax[0, 1])\n",
    "sns.histplot(x=group[group['year'] == 2013]['MW'], kde=True, ax=ax[0, 2])\n",
    "sns.histplot(x=group[group['year'] == 2014]['MW'], kde=True, ax=ax[0, 3])\n",
    "sns.histplot(x=group[group['year'] == 2015]['MW'], kde=True, ax=ax[1, 0])\n",
    "sns.histplot(x=group[group['year'] == 2016]['MW'], kde=True, ax=ax[1, 1])\n",
    "sns.histplot(x=group[group['year'] == 2017]['MW'], kde=True, ax=ax[1, 2])\n",
    "sns.histplot(x=group[group['year'] == 2018]['MW'], kde=True, ax=ax[1, 3])\n",
    "sns.histplot(x=group[group['year'] == 2019]['MW'], kde=True, ax=ax[2, 0])\n",
    "sns.histplot(x=group[group['year'] == 2020]['MW'], kde=True, ax=ax[2, 1])\n",
    "sns.histplot(x=group[group['year'] == 2021]['MW'], kde=True, ax=ax[2, 2])\n",
    "ax[2, 3].set_visible(False)\n",
    "\n",
    "\n",
    "ax[0, 0].set_title(\"Monthly average Wind power distribution of 2011\")\n",
    "ax[0, 1].set_title(\"Monthly average Wind power distribution of 2012\")\n",
    "ax[0, 2].set_title(\"Monthly average Wind power distribution of 2013\")\n",
    "ax[0, 3].set_title(\"Monthly average Wind power distribution of 2014\")\n",
    "ax[1, 0].set_title(\"Monthly average Wind power distribution of 2015\")\n",
    "ax[1, 1].set_title(\"Monthly average Wind power distribution of 2016\")\n",
    "ax[1, 2].set_title(\"Monthly average Wind power distribution of 2017\")\n",
    "ax[1, 3].set_title(\"Monthly average Wind power distribution of 2018\")\n",
    "ax[2, 0].set_title(\"Monthly average Wind power distribution of 2019\")\n",
    "ax[2, 1].set_title(\"Monthly average Wind power distribution of 2020\")\n",
    "ax[2, 2].set_title(\"Monthly average Wind power distribution of 2021\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- These distributions do not seem to follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the time series column as index:\n",
    "df.set_index('dt', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Class below has a method that can transform a dataset with a Time series structure into a dataset that can be used in a supervised manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proprocessing class \n",
    "class Preprocessamento:\n",
    "\n",
    "    def timeseries_to_supervised(self, df, n_features, n_target):\n",
    "        n_linhas = 0\n",
    "        colunas_features = ['var(t - {})'.format(str(i)) for i in range(n_features, -1, -1) if i != 0]\n",
    "        colunas_target = ['var(t)' if i==0 else 'var(t + {})'.format(str(i)) for i in range(0, n_target)]\n",
    "        colunas_total = colunas_features + colunas_target\n",
    "        lista=[]\n",
    "        \n",
    "        while n_linhas <= len(df) - n_target - n_features:\n",
    "            quantidade_de_features_iteracao = df.iloc[n_linhas:n_linhas + n_features].values\n",
    "            quantidade_de_target_iteracao = df.iloc[n_linhas + n_features: n_linhas + n_features + n_target]\n",
    "            \n",
    "            linha = np.concatenate([quantidade_de_features_iteracao, quantidade_de_target_iteracao], axis=0)\n",
    "            linha_reshape = linha.reshape(1, -1)\n",
    "            lista.append(linha_reshape[0])\n",
    "    \n",
    "            n_linhas += 1\n",
    "        df_iter = pd.DataFrame(lista, columns=colunas_total)\n",
    "\n",
    "        return df_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that can plot several metrics:\n",
    "def metricas(X_test, y_test, models):\n",
    "    for name, model in models.items():\n",
    "        if name == 'LSTM':\n",
    "            y_pred = []\n",
    "            for i in range(len(X_test)):\n",
    "                X_test_linha = X_test[i, 0:]\n",
    "                X_test_reshaped = X_test_linha.reshape(1, 1, len(X_test_linha))\n",
    "                predicoes = model.predict(X_test_reshaped, batch_size=1, verbose=0)\n",
    "                retorno = [x for x in predicoes[0]][0]\n",
    "                y_pred.append(retorno)\n",
    "            y_pred = np.array(y_pred)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        coef_pearson = r_regression(y_pred.reshape(-1, 1), y_test)[0]\n",
    "        print(f'Mean squared error: {mse}')\n",
    "        print(f'Root Mean squared error: {rmse}')\n",
    "        print(f'Coef de pearson: {coef_pearson}')\n",
    "        print('###########################################\\n')\n",
    "\n",
    "        return mse, rmse, coef_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splintting into train an test datasets:\n",
    "percentagem_treino_inicial = 0.8\n",
    "\n",
    "limite_treino_inicial = int(len(df)*percentagem_treino_inicial)\n",
    "df_train_inicial = df.iloc[0:limite_treino_inicial]\n",
    "df_test = df.iloc[limite_treino_inicial:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and validation datasets:\n",
    "percentagem_treino = 0.8\n",
    "\n",
    "limite_treino = int(len(df_train_inicial)*percentagem_treino)\n",
    "df_train = df_train_inicial.iloc[0:limite_treino]\n",
    "df_val = df_train_inicial.iloc[limite_treino:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intance of Preprocessing class:\n",
    "prep_obj = Preprocessamento()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a window of 20 and a horizon of 10. In other words, we are going to use 20 features to predict ten timesteps ahead in time. Furthemore, it is important to say that we will make a model for each timestep ahead, so it is going to be 10 models for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the time series into a supervised problem:\n",
    "n_features = 20\n",
    "n_target = 10\n",
    "\n",
    "df_train_supervised = prep_obj.timeseries_to_supervised(df_train, n_features, n_target)\n",
    "df_val_supervised = prep_obj.timeseries_to_supervised(df_val, n_features, n_target)\n",
    "df_test_supervised = prep_obj.timeseries_to_supervised(df_test, n_features, n_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some descriptive statistics about the features:\n",
    "df_train_supervised.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- All of the features have approxmately the same mean and Standard Deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test fetures and targets:\n",
    "X_train, y_train = df_train_supervised.values[:, 0:n_features], df_train_supervised.values[:, n_features:]\n",
    "X_val, y_val = df_val_supervised.values[:, 0:n_features], df_val_supervised.values[:, n_features:]\n",
    "X_test, y_test = df_test_supervised.values[:, 0:n_features], df_test_supervised.values[:, n_features:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features shape:\n",
    "print('Feature shapes:')\n",
    "print(f'Training: {X_train.shape}')\n",
    "print(f'Validation: {X_val.shape}')\n",
    "print(f'Test: {X_test.shape}')\n",
    "print('#########################\\n')\n",
    "\n",
    "# Target shape:\n",
    "print('Target shapes:')\n",
    "print(f'Training: {y_train.shape}')\n",
    "print(f'Validation: {y_val.shape}')\n",
    "print(f'Test: {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1) Standard Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition:\n",
    "- The Standard Scaler is a technique that rescales the distribution of a variable so that the mean of the observed sample is 0 and the standard deviation is 1. It is particularly useful for algorithms that rely on distance measures, such as K-means and K-nearest neighbors (KNN). Additionally, it is a recommended choice for algorithms based on neural networks.\n",
    "\n",
    "OBS: Standard Scaler can perform slightly worst than the other transformations because it assumes that the data is normally distributed. However you can still standardize your data.\n",
    "\n",
    "Matematical Definition:\n",
    "\n",
    "$X_{new_{i}} = \\frac{X_{i} - \\hat{\\mu}_{i}}{\\sigma_{i}}$\n",
    "\n",
    "- $\\mu:$ Mean of the sample.\n",
    "- $\\sigma:$ Standard Deviation of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a Standard Scaler object:\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train)\n",
    "\n",
    "# Transforming all the sets:\n",
    "X_train_std = std_scaler.transform(X_train) \n",
    "X_val_std = std_scaler.transform(X_val)\n",
    "X_test_std = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nested directories: \n",
    "def make_directory(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except FileExistsError:\n",
    "        print(\"File already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory name:\n",
    "directory = dt.now().strftime(\"%Y-%m-%d__%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Multilayer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that structure a simple neural network architecture:\n",
    "def mlp_simples(device):\n",
    "\n",
    "    with tf.device(device):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(100, activation='relu', input_shape=X_train.shape[1:]),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a directory for MLP models:\n",
    "path_modelos_mlp_simples = \"modelos_mlp_simples/{}\".format(directory)\n",
    "make_directory(path_modelos_mlp_simples)\n",
    "\n",
    "# Training a MLP:\n",
    "for target in range(n_target):\n",
    "    \n",
    "    # EarlyStopping callback:\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    checkpoint_mlp = tf.keras.callbacks.ModelCheckpoint('{}/model{}_mlp.h5'.format(path_modelos_mlp_simples, target))\n",
    "    model_mlp = mlp_simples('/device:GPU:0')\n",
    "    history_2 = model_mlp.fit(X_train_std, y_train[:, target], epochs=60, \n",
    "                    validation_data=(X_val_std, y_val[:, target]), callbacks=[earlystopping, checkpoint_mlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for each horizon for MLP:\n",
    "resultados_mlp = []\n",
    "path = \"modelos_mlp_simples/{}\".format(directory)\n",
    "\n",
    "for target, i in enumerate(os.listdir(path)):\n",
    "    path_temp = path + \"/\" + str(i)\n",
    "    modelo_carregado = tf.keras.models.load_model(path_temp)\n",
    "    mse, rmse, coef_p = metricas(X_test_std, y_test[:, target], {'mlp':modelo_carregado})\n",
    "    resultados_mlp.append([mse, rmse, coef_p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Random Forest models for each horizon:\n",
    "resultados_rnd = []\n",
    "for target in range(n_target):\n",
    "    rnd_model = RandomForestRegressor(random_state=42)\n",
    "    rnd_model.fit(X_train_std, y_train[:, target])\n",
    "    print(f'Random Forest - {target}:')\n",
    "    mse, rmse, coef_p = metricas(X_test_std, y_test[:, target], {'rnd':rnd_model})\n",
    "    resultados_rnd.append([mse, rmse, coef_p])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training XGBoost models for each horizon:\n",
    "resultados_xgb = []\n",
    "for target in range(n_target):\n",
    "    xgb_model = XGBRegressor(random_state=42)\n",
    "    xgb_model.fit(X_train_std, y_train[:, target])\n",
    "\n",
    "    print(f'XGBoost - {target}:')\n",
    "    mse, rmse, coef_p = metricas(X_test_std, y_test[:, target], {'xgb':xgb_model})\n",
    "    resultados_xgb.append([mse, rmse, coef_p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4) Long Short term memory Neural Network (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the features into a 3D Matrix:\n",
    "X_train_lstm = X_train_std.reshape(X_train_std.shape[0], 1, X_train_std.shape[1])\n",
    "X_val_lstm = X_val_std.reshape(X_val_std.shape[0], 1, X_val_std.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM's architecture:\n",
    "def model_LSTM(device):\n",
    "    with tf.device(device):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(LSTM(1, batch_input_shape=(1, X_train_lstm.shape[1], X_train_lstm.shape[2]), stateful=True))\n",
    "        model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a LSTM for each horizon:\n",
    "path = \"modelos_lstm/{}\".format(directory)\n",
    "make_directory(path)\n",
    "\n",
    "for target in range(n_target):\n",
    "    \n",
    "    # EarlyStopping callback:\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"{}/lstm_model{}.h5\".format(path, target))\n",
    "    modelo_lstm = model_LSTM('/device:GPU:0')\n",
    "\n",
    "    for i in range(120):\n",
    "        history = modelo_lstm.fit(X_train_lstm, y_train[:, target], epochs=1, batch_size=1, verbose=1, shuffle=False, \n",
    "        validation_data=(X_val_lstm, y_val[:, target]), validation_batch_size=1, callbacks=[earlystop, checkpoint])\n",
    "        modelo_lstm.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the LSTM:\n",
    "resultados_lstm = []\n",
    "path = \"modelos_lstm/{}\".format(directory)\n",
    "\n",
    "for target, i in enumerate(os.listdir(path)):\n",
    "    path_temp = path + \"/\" + str(i)\n",
    "    modelo_carregado = tf.keras.models.load_model(path_temp)\n",
    "    \n",
    "    print(f'LSTM - {target}:')\n",
    "    mse, rmse, coef_p = metricas(X_test_std, y_test[:, target], {'LSTM':modelo_carregado})\n",
    "    resultados_lstm.append([mse, rmse, coef_p])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5) Convolutional 1D Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping variables to fit as Input for a CNN 1D:\n",
    "X_train_conv = np.array(X_train_std).reshape(X_train_std.shape[0], X_train_std.shape[1], 1)\n",
    "X_val_conv = np.array(X_val_std).reshape(X_val_std.shape[0], X_val_std.shape[1], 1)\n",
    "X_test_conv = np.array(X_test_std).reshape(X_test_std.shape[0], X_test_std.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 1D:\n",
    "def timeseries_model_conv(device):\n",
    "    with tf.device(device):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Conv1D(filters=64, kernel_size=7, activation=\"relu\", padding=\"same\", input_shape=(X_train_conv.shape[1], 1)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\"))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=256, kernel_size=3, activation=\"relu\", padding=\"same\"))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "        model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a 1D CNN for each horizon:\n",
    "path = \"modelos_cnn/{}\".format(directory)\n",
    "make_directory(path)\n",
    "\n",
    "for target in range(n_target):\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"{}/cnn_model{}.h5\".format(path, target))\n",
    "    model_conv = timeseries_model_conv(\"/device:GPU:0\")\n",
    "\n",
    "    history_conv = model_conv.fit(X_train_conv, y_train[:, target], validation_data=(X_val_conv, y_val[:, target]), \n",
    "                  epochs=120, callbacks=[earlystop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics fr the 1D CNN:\n",
    "resultados_cnn = []\n",
    "path = 'modelos_cnn/{}'.format(directory)\n",
    "\n",
    "for index, model in enumerate(os.listdir(path)):\n",
    "    path_temp = path + \"/\" + model\n",
    "    modelo_carregado = tf.keras.models.load_model(path_temp)\n",
    "    \n",
    "    print(f'CNN 1D - {target}:')\n",
    "    mse, rmse, coef_p = metricas(X_test_conv, y_test[:, target], {'CNN':modelo_carregado})\n",
    "    resultados_cnn.append([mse, rmse, coef_p])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the result figures:\n",
    "resultados_mlp = np.array(resultados_mlp)\n",
    "resultados_rnd = np.array(resultados_rnd)\n",
    "resultados_xgb = np.array(resultados_xgb)\n",
    "resultados_lstm = np.array(resultados_lstm)\n",
    "resultados_cnn = np.array(resultados_cnn)\n",
    "metricas_iterar = ['MSE', 'RMSE', 'COEF_PEARSON']\n",
    "\n",
    "# Creating a directory to store all the images:\n",
    "path = 'imagens/{}'.format(directory)\n",
    "make_directory(path)\n",
    "\n",
    "for index, nome in enumerate(metricas_iterar):\n",
    "    resultados_mlp_reshaped = resultados_mlp[:, index].reshape(-1, 1)\n",
    "    resultados_rnd_reshaped = resultados_rnd[:, index].reshape(-1, 1)\n",
    "    resultados_xgb_reshaped = resultados_xgb[:, index].reshape(-1, 1)\n",
    "    resultados_lstm_reshaped = resultados_lstm[:, index].reshape(-1, 1)\n",
    "    resultados_cnn_reshaped = resultados_cnn[:, index].reshape(-1, 1)\n",
    "\n",
    "    array_resultados_mse = np.concatenate([resultados_mlp_reshaped, resultados_rnd_reshaped, \n",
    "                                          resultados_xgb_reshaped, resultados_lstm_reshaped,\n",
    "                                          resultados_cnn_reshaped], axis=1)\n",
    "\n",
    "    df_resultados = pd.DataFrame(array_resultados_mse, columns=['MLP', 'RANDOM_FOREST', 'XGBOOST', 'LSTM', 'CNN']).reset_index(names='Horizontes')\n",
    "    df_resultados_melted = df_resultados.melt(id_vars='Horizontes', value_name=nome, var_name='Modelos')\n",
    "\n",
    "    fig_resultado = px.line(df_resultados_melted, x='Horizontes', y=nome, hover_data=['Modelos'], color='Modelos',\n",
    "    title='{} - 12 Horizontes'.format(nome))\n",
    "    \n",
    "    pl.io.write_image(fig=fig_resultado, file='imagens\\{}\\{}.jpg'.format(directory, nome), width=1000, height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('IBITU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29505a5d549ee8a9c5fb3342af4d022dbe28e4e6bd678c5f06d5cb8a71ed7c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
